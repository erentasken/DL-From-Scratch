# Deep Learning Exercises

A collection of deep learning implementations built from scratch using NumPy, including Convolutional Neural Networks (CNN), Feed-Forward Neural Networks, and Recurrent Neural Networks with regularization techniques.

## ğŸ“š Project Overview

This repository contains implementations of various neural network architectures and deep learning concepts, built as educational exercises. Each component is implemented from scratch to provide a deep understanding of the underlying mathematics and algorithms.

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ CNN/                              # Convolutional Neural Network implementation
â”‚   â”œâ”€â”€ Layers/                       # CNN layer implementations
â”‚   â”‚   â”œâ”€â”€ Conv.py                   # Convolutional layer
â”‚   â”‚   â”œâ”€â”€ Pooling.py                # Pooling layer
â”‚   â”‚   â”œâ”€â”€ Flatten.py                # Flatten layer
â”‚   â”‚   â”œâ”€â”€ FullyConnected.py         # Fully connected layer
â”‚   â”‚   â”œâ”€â”€ ReLU.py                   # ReLU activation
â”‚   â”‚   â””â”€â”€ SoftMax.py                # SoftMax activation
â”‚   â”œâ”€â”€ Optimization/                 # Optimization algorithms
â”‚   â”‚   â”œâ”€â”€ Optimizers.py             # SGD, Adam, etc.
â”‚   â”‚   â””â”€â”€ Loss.py                   # Loss functions
â”‚   â”œâ”€â”€ NeuralNetwork.py              # Main CNN architecture
â”‚   â””â”€â”€ NeuralNetworkTests.py         # Comprehensive test suite
â”‚
â”œâ”€â”€ FeedForwardNeuralNetwork/         # Feed-forward neural network
â”‚   â”œâ”€â”€ Layers/                       # Layer implementations
â”‚   â”œâ”€â”€ Optimization/                 # Optimizers and loss functions
â”‚   â”œâ”€â”€ NeuralNetwork.py              # Main network class
â”‚   â”œâ”€â”€ NeuralNetworkTests.py         # Test suite
â”‚   â””â”€â”€ main.ipynb                    # Example usage notebook
â”‚
â”œâ”€â”€ Regularization_Recurrent/         # Recurrent networks with regularization
â”‚   â”œâ”€â”€ Layers/                       # RNN layer implementations
â”‚   â”œâ”€â”€ Optimization/                 # Optimizers with regularization
â”‚   â”œâ”€â”€ Models/                       # Model architectures
â”‚   â”œâ”€â”€ Data/                         # Dataset utilities
â”‚   â””â”€â”€ TrainLeNet.py                 # Training script
â”‚
â”œâ”€â”€ numpy/                            # NumPy pattern generation exercises
â”‚   â”œâ”€â”€ pattern.py                    # Pattern generators
â”‚   â”œâ”€â”€ generator.py                  # Data generators
â”‚   â”œâ”€â”€ NumpyTests.py                 # Unit tests
â”‚   â””â”€â”€ exercise_data/                # Exercise datasets
â”‚
â””â”€â”€ playground.ipynb                  # Experimental notebook
```

## ğŸš€ Features

### CNN Implementation
- **Convolutional Layers**: Forward and backward propagation with stride and padding support
- **Pooling Layers**: Max and average pooling with configurable kernel sizes
- **Activation Functions**: ReLU, SoftMax, and more
- **Weight Initialization**: He, Xavier, and custom initializers
- **Optimizers**: SGD, SGD with Momentum, Adam
- **Comprehensive Testing**: Over 1000 lines of unit tests

### Feed-Forward Neural Network
- Multi-layer perceptron implementation
- Flexible architecture configuration
- Various activation functions
- Backpropagation from scratch
- Training and testing utilities

### Regularization & Recurrent Networks
- Dropout and L2 regularization
- Recurrent neural network layers
- LeNet architecture implementation
- Advanced optimization techniques

### NumPy Exercises
- Pattern generation (checkerboard, circles, etc.)
- Data manipulation and transformation
- Array operations and broadcasting
- Comprehensive test suite

## ğŸ“‹ Requirements

```
numpy==1.26.4
matplotlib
scipy
scikit-learn
scikit-image
tabulate
```

## ğŸ”§ Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/deep-learning-exercises.git
cd deep-learning-exercises
```

2. Create a virtual environment (recommended):
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ’» Usage

### CNN Example

```python
from CNN.NeuralNetwork import NeuralNetwork
from CNN.Layers import Conv, ReLU, Pooling, Flatten, FullyConnected, SoftMax
from CNN.Optimization import Optimizers, Loss

# Initialize network
optimizer = Optimizers.Adam(0.001, 0.9, 0.999)
nn = NeuralNetwork(optimizer, weight_initializer, bias_initializer)

# Build architecture
nn.append_layer(Conv.Conv((1, 5, 5), 3, stride=1, padding=2))
nn.append_layer(ReLU.ReLU())
nn.append_layer(Pooling.Pooling((2, 2), stride=2))
nn.append_layer(Flatten.Flatten())
nn.append_layer(FullyConnected.FullyConnected(128, 10))
nn.append_layer(SoftMax.SoftMax())

# Set loss and train
nn.loss_layer = Loss.CrossEntropyLoss()
nn.train(iterations=1000)
```

### Feed-Forward Network Example

```python
from FeedForwardNeuralNetwork.NeuralNetwork import NeuralNetwork
from FeedForwardNeuralNetwork.Layers import FullyConnected, ReLU, SoftMax
from FeedForwardNeuralNetwork.Optimization import Optimizers, Loss

# Create network
nn = NeuralNetwork(optimizer, weight_initializer, bias_initializer)

# Add layers
nn.append_layer(FullyConnected.FullyConnected(784, 256))
nn.append_layer(ReLU.ReLU())
nn.append_layer(FullyConnected.FullyConnected(256, 10))
nn.append_layer(SoftMax.SoftMax())

# Train
nn.loss_layer = Loss.CrossEntropyLoss()
nn.train(iterations=5000)
```

## ğŸ§ª Running Tests

Each module includes comprehensive unit tests:

```bash
# Test CNN implementation
cd CNN
python -m pytest NeuralNetworkTests.py

# Test Feed-Forward Network
cd FeedForwardNeuralNetwork
python -m pytest NeuralNetworkTests.py

# Test NumPy exercises
cd numpy
python -m pytest NumpyTests.py
```

## ğŸ“Š Project Highlights

- **From Scratch Implementation**: All components built without high-level frameworks (TensorFlow/PyTorch)
- **Educational Focus**: Clear, readable code with detailed comments
- **Comprehensive Testing**: Extensive test suites ensure correctness
- **Modular Design**: Easily extensible architecture
- **Mathematical Rigor**: Proper implementation of gradients and optimization algorithms

## ğŸ¯ Learning Objectives

This project demonstrates understanding of:
- Neural network architectures and forward/backward propagation
- Convolutional operations and their mathematical foundations
- Optimization algorithms (SGD, Momentum, Adam)
- Weight initialization strategies
- Activation functions and their derivatives
- Loss functions and gradient computation
- NumPy broadcasting and vectorization
- Software engineering best practices for ML projects

## ğŸ¤ Contributing

This is an educational project, but suggestions and improvements are welcome! Feel free to:
- Report bugs
- Suggest enhancements
- Submit pull requests
- Improve documentation

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

**Eren**

## ğŸ™ Acknowledgments

- Built as part of a Deep Learning course curriculum
- Inspired by various deep learning textbooks and papers
- Test cases provided by course instructors

## ğŸ“š Resources

- [Deep Learning Book by Ian Goodfellow](https://www.deeplearningbook.org/)
- [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)
- [Neural Networks and Deep Learning by Michael Nielsen](http://neuralnetworksanddeeplearning.com/)

---

â­ If you find this repository helpful, please consider giving it a star!
